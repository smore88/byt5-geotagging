{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jxQONAqxzOE",
        "outputId": "50233d1b-db4d-4bea-ad25-24a58ec2ce2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.29.1\n",
            "  Obtaining dependency information for transformers==4.29.1 from https://files.pythonhosted.org/packages/e8/b5/ddb16f9de207e6571ab7cc5db0cc538fa2d6d91cf024565496462af4c1ce/transformers-4.29.1-py3-none-any.whl.metadata\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl.metadata (112 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.63.2\n",
            "  Downloading tqdm-4.63.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.4\n",
            "  Downloading pandas-1.4.4.tar.gz (4.9 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting wandb\n",
            "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/35/d3/6bfe29e4ba1eb2400d478caf8e3af9a1c366390390069cda59a7c6bf6063/wandb-0.16.1-py3-none-any.whl.metadata\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: filelock in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (2022.7.9)\n",
            "Requirement already satisfied: requests in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from transformers==4.29.1) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from pandas==1.4.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from pandas==1.4.4) (2023.3.post1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from wandb) (8.0.4)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/8d/c4/82b858fb6483dfb5e338123c154d19c043305b01726a67d89532b8f8f01b/GitPython-3.1.40-py3-none-any.whl.metadata\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/aa/39/a40c841782b775eec1602a82387b4e91322ccafd842fd60fc4deb9f13f7d/sentry_sdk-1.39.1-py2.py3-none-any.whl.metadata\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/c9/17/7f9d5ddf4cfc4386e74565ccf63b8381396336e4629bb165b52b803ceddb/setproctitle-1.3.3-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
            "  Downloading setproctitle-1.3.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /Users/shubham/anaconda3/lib/python3.11/site-packages (from wandb) (68.0.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
            "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
            "  Obtaining dependency information for protobuf!=4.21.0,<5,>=3.19.0 from https://files.pythonhosted.org/packages/e6/db/7b2edc72807d45d72f9db42f3eb86ddaf37f9e55d923159b1dbfc9d835bc/protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
            "  Using cached protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: six>=1.4.0 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: fsspec in /Users/shubham/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.1) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.1) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.29.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.29.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.29.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.29.1) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp311-cp311-macosx_10_9_universal2.whl (16 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pandas\n",
            "  Building wheel for pandas (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pandas: filename=pandas-1.4.4-cp311-cp311-macosx_11_0_arm64.whl size=9810607 sha256=3742828984686d10fe471022ff43c53e97929db413095a519fea953f42a706bc\n",
            "  Stored in directory: /Users/shubham/Library/Caches/pip/wheels/56/ff/8e/5497c17f51e61a95c9cf4c58914222f311ca968796ee52d430\n",
            "Successfully built pandas\n",
            "Installing collected packages: tqdm, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, pandas, gitdb, transformers, GitPython, wandb\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.32.1\n",
            "    Uninstalling transformers-4.32.1:\n",
            "      Successfully uninstalled transformers-4.32.1\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 pandas-1.4.4 protobuf-4.25.1 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 tqdm-4.63.2 transformers-4.29.1 wandb-0.16.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages\n",
        "%pip install transformers==4.29.1 tqdm==4.63.2 pandas==1.4.4 wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa4vndBrimra",
        "outputId": "aa1f42d3-8659-4154-dd9e-8c6eba037e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2EsT4lghapg",
        "outputId": "1803c302-a682-4e77-c66f-40a391699bcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'byt5-geotagging'...\n",
            "remote: Enumerating objects: 256, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 256 (delta 63), reused 77 (delta 36), pack-reused 146\u001b[K\n",
            "Receiving objects: 100% (256/256), 13.04 MiB | 24.23 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /Users/shubham/anaconda3/lib/python3.11/site-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /Users/shubham/anaconda3/lib/python3.11/site-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /Users/shubham/anaconda3/lib/python3.11/site-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /Users/shubham/anaconda3/lib/python3.11/site-packages (from gdown) (4.63.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/shubham/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-4.7.1\n",
            "zsh:1: no matches found: https://drive.google.com/u/2/uc?id=1thkE-hgT3sDtZqILZH17Hyayy0hkk_jh\n",
            "tar: Error opening archive: Failed to open 'challenge_1.tar.gz'\n",
            "head: data_sample_lc/c_46.json: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Clone the repository for geotagging with BYT5\n",
        "!git clone https://github.com/Yachay-AI/byt5-geotagging\n",
        "\n",
        "\n",
        "# Install gdown for Google Drive downloads\n",
        "!pip install gdown\n",
        "\n",
        "# Download the dataset\n",
        "!gdown https://drive.google.com/u/2/uc?id=1thkE-hgT3sDtZqILZH17Hyayy0hkk_jh&export=download\n",
        "\n",
        "# Unzip the downloaded dataset\n",
        "!tar xvf challenge_1.tar.gz > /dev/null\n",
        "\n",
        "# Read sample data\n",
        "!head data_sample_lc/c_46.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad1kJoI4hcVB",
        "outputId": "749f0f78-0524-4d0a-9bdb-f62bc4b3986c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "731994 train.csv\n",
            "81623 test.csv\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize an empty list to hold dataframes\n",
        "df_list = []\n",
        "\n",
        "# Loop through each JSON file and append it to df_list\n",
        "for fn in os.listdir(\"data_sample_lc\"):\n",
        "  df_list.append(pd.read_json(f\"data_sample_lc/{fn}\", lines=True))\n",
        "\n",
        "# Concatenate all dataframes in df_list\n",
        "df = pd.concat(df_list)\n",
        "\n",
        "# Extract latitude and longitude from 'coordinates' column\n",
        "df['lat'] = [x[1] for x in df['coordinates']]\n",
        "df['lon'] = [x[0] for x in df['coordinates']]\n",
        "\n",
        "# Drop the 'coordinates' column\n",
        "df.drop('coordinates', axis=1, inplace=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1.0)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "df.iloc[:len(df)*9//10].to_csv('train.csv')\n",
        "df.iloc[len(df)*9//10:].to_csv('test.csv')\n",
        "\n",
        "# Count the number of lines in train.csv and test.csv\n",
        "!wc -l train.csv\n",
        "!wc -l test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7swmfDechfIn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load cluster data\n",
        "cluster_df = pd.read_csv('byt5-geotagging/cluster_df.csv')\n",
        "\n",
        "# Save the clustering model\n",
        "with open('clustering.pkl', 'wb') as fout:\n",
        "  pickle.dump((cluster_df, []), fout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZhJc7tshgcv",
        "outputId": "4908262e-a0df-41e8-f9a1-ae17eeb3c3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:root:start\n",
            "start\n",
            "INFO:root:finish reading test file\n",
            "finish reading test file\n",
            "Warning, dropping 1 NaN rows\n",
            "INFO:root:finish reading train file\n",
            "finish reading train file\n",
            "INFO:root:Index(['Unnamed: 0', 'text', 'lat', 'lon', 'coordinates'], dtype='object')\n",
            "Index(['Unnamed: 0', 'text', 'lat', 'lon', 'coordinates'], dtype='object')\n",
            "Downloading (…)okenizer_config.json: 100% 2.59k/2.59k [00:00<00:00, 11.9MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 698/698 [00:00<00:00, 3.14MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.50k/2.50k [00:00<00:00, 10.7MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.20G/1.20G [00:39<00:00, 30.4MB/s]\n",
            "Some weights of the model checkpoint at google/byt5-small were not used when initializing T5EncoderModel: ['decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'lm_head.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight']\n",
            "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "INFO:root:ByT5_classifier(\n",
            "  (byt5): T5EncoderModel(\n",
            "    (shared): Embedding(384, 1472)\n",
            "    (encoder): T5Stack(\n",
            "      (embed_tokens): Embedding(384, 1472)\n",
            "      (block): ModuleList(\n",
            "        (0): T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "                (relative_attention_bias): Embedding(32, 6)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1-3): 3 x T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=1472, out_features=3000, bias=True)\n",
            ")\n",
            "ByT5_classifier(\n",
            "  (byt5): T5EncoderModel(\n",
            "    (shared): Embedding(384, 1472)\n",
            "    (encoder): T5Stack(\n",
            "      (embed_tokens): Embedding(384, 1472)\n",
            "      (block): ModuleList(\n",
            "        (0): T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "                (relative_attention_bias): Embedding(32, 6)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1-3): 3 x T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=1472, out_features=3000, bias=True)\n",
            ")\n",
            "INFO:root:setting learning rate to 0.001\n",
            "setting learning rate to 0.001\n",
            "  0% 0/1500 [00:00<?, ?it/s]INFO:root:Epoch 0 training loss 8.008275032043457\n",
            "Epoch 0 training loss 8.008275032043457\n",
            "INFO:root:Epoch 0 eval loss 8.003212571144104  accuracy 0.0 true distance avg 8287.892578125 true distance median 7496.5576171875\n",
            "Epoch 0 eval loss 8.003212571144104  accuracy 0.0 true distance avg 8287.892578125 true distance median 7496.5576171875\n",
            "INFO:root:saved to models/byt5-class-0\n",
            "saved to models/byt5-class-0\n",
            "  1% 11/1500 [00:33<35:46,  1.44s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run the training script\n",
        "# The parameters here are chosen to show a small training run on a small subset of data\n",
        "!python byt5-geotagging/train_model.py --train_input_file train.csv --test_input_file test.csv --do_train true --do_test true --load_clustering ./ --device cuda --batch_size 64 --keep_layer_count 4 --max_train 96000 --max_test 640\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDY5kaY7hhZl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run the testing script\n",
        "# For demo purpose, a small subset of validation set is used\n",
        "!python byt5-geotagging/train_model.py --train_input_file train.csv --test_input_file test.csv --do_test true --load_clustering ./ --load_model_dir models/byt5-class-0 --device cuda --batch_size 32  --max_train 96000 --max_test 640\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDzMsI92htqN"
      },
      "outputs": [],
      "source": [
        "cd byt5-geotagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRMdQXHEhiRA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load trained model and tokenizer\n",
        "device = 'cuda'\n",
        "byt5 = torch.load('../models/byt5-class-0')\n",
        "byt5_tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
        "\n",
        "# Make a prediction\n",
        "text = 'I live in New York'\n",
        "inputs = byt5_tokenizer(text, return_tensors='pt')['input_ids'].unsqueeze(0)\n",
        "logits = byt5.to(device)(inputs.to(device))\n",
        "predicted_cluster = logits.argmax()\n",
        "confidence = torch.nn.functional.softmax(logits, dim=-1).max().item()\n",
        "predicted_location = cluster_df.iloc[predicted_cluster.item()]\n",
        "\n",
        "# Output predicted location and confidence\n",
        "print(predicted_location['lat'], predicted_location['lng'], confidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljHoXvAOilvO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
